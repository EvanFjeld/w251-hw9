{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cace261f",
      "metadata": {
        "id": "cace261f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d693cd6c",
      "metadata": {
        "id": "d693cd6c"
      },
      "outputs": [],
      "source": [
        "SEED=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "363e2b75",
      "metadata": {
        "id": "363e2b75"
      },
      "outputs": [],
      "source": [
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0d479743",
      "metadata": {
        "id": "0d479743"
      },
      "outputs": [],
      "source": [
        "START_EPOCH = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ec40e9",
      "metadata": {
        "id": "c8ec40e9"
      },
      "source": [
        "### Set the architecture to resnet 18 below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "97b1a0aa",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "97b1a0aa"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "ARCH = 'resnet18'# set the architecture to RESNET 18\n",
        "# please look up how to do that\n",
        "########################\n",
        "EPOCHS = 20\n",
        "LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "PRINT_FREQ = 50\n",
        "TRAIN_BATCH=128\n",
        "VAL_BATCH=128\n",
        "WORKERS=2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e86c3188",
      "metadata": {
        "id": "e86c3188"
      },
      "source": [
        "### Check if cuda is available here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "63a0499b",
      "metadata": {
        "id": "63a0499b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825a0a0a-227a-4563-c235-66235c3485d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available: True\n"
          ]
        }
      ],
      "source": [
        "# check if cuda is available in this cell\n",
        "# if it is not available, you should not go forward!\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83c35f6",
      "metadata": {
        "id": "f83c35f6"
      },
      "source": [
        "### Assign your GPU below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "252a08a3",
      "metadata": {
        "id": "252a08a3"
      },
      "outputs": [],
      "source": [
        "# Assign your GPU in this cell\n",
        "GPU = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "d8e7ec23",
      "metadata": {
        "id": "d8e7ec23"
      },
      "outputs": [],
      "source": [
        "# set your active device to your GPU in this cell\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "1e5ddf20",
      "metadata": {
        "id": "1e5ddf20"
      },
      "outputs": [],
      "source": [
        "# enable algorithm optimization\n",
        "cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cdd6e12",
      "metadata": {
        "id": "7cdd6e12"
      },
      "source": [
        "### Fill in the heart of the train section below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "70dbd4c4",
      "metadata": {
        "id": "70dbd4c4"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses, top1, top5],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    ######################\n",
        "    # switch model to train mode here\n",
        "    model.train()\n",
        "    ################\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        #####################\n",
        "        # send the images to cuda device\n",
        "        images = images.to(device)\n",
        "        # send the target to cuda device\n",
        "        target = target.to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "\n",
        "        # compute loss \n",
        "        loss = criterion(output, target)\n",
        "\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        \n",
        "        #### zero out gradients in the optimier\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ## backprop!\n",
        "        loss.backward()\n",
        "        \n",
        "        # update the weights!\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            progress.display(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb3b62e",
      "metadata": {
        "id": "deb3b62e"
      },
      "source": [
        "#### Fill in the validate section below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "a3f276cc",
      "metadata": {
        "id": "a3f276cc"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader),\n",
        "        [batch_time, losses, top1, top5],\n",
        "        prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            \n",
        "            \n",
        "            ### send the images and target to cuda\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % PRINT_FREQ == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        # TODO: this should also be done with the ProgressMeter\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return [top1.avg, loss]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe49226",
      "metadata": {
        "id": "fbe49226"
      },
      "source": [
        "### Save the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "ff8a4159",
      "metadata": {
        "id": "ff8a4159"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    # save the model state!\n",
        "    torch.save(state, filename)\n",
        "    \n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "1cd7ea3a",
      "metadata": {
        "id": "1cd7ea3a"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "e1da87ab",
      "metadata": {
        "id": "e1da87ab"
      },
      "outputs": [],
      "source": [
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "00211030",
      "metadata": {
        "id": "00211030"
      },
      "outputs": [],
      "source": [
        "# if we are adjusting the LR manually use this\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = LR * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "da2c1382",
      "metadata": {
        "id": "da2c1382"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "5c29e7a1",
      "metadata": {
        "id": "5c29e7a1"
      },
      "outputs": [],
      "source": [
        "cifar_mean_RGB = [0.4914, 0.4822, 0.4465]\n",
        "cifar_std_RGB = [0.2023, 0.1994, 0.2010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "e1a61c02",
      "metadata": {
        "id": "e1a61c02"
      },
      "outputs": [],
      "source": [
        "normalize = transforms.Normalize(mean=cifar_mean_RGB, std=cifar_std_RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "47dd3e49",
      "metadata": {
        "id": "47dd3e49"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 32\n",
        "# IMG_SIZE = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de4387cb",
      "metadata": {
        "id": "de4387cb"
      },
      "source": [
        "### Initialize the model using the architecture you selected above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "b1abcc33",
      "metadata": {
        "id": "b1abcc33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49633dc-d5f1-4d0c-b967-ee21183d8ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# select the model\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db1bb69",
      "metadata": {
        "id": "2db1bb69"
      },
      "source": [
        "### Send the model to the cuda device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "7d23ccb4",
      "metadata": {
        "id": "7d23ccb4"
      },
      "outputs": [],
      "source": [
        "# send the model to the cuda device.. \n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47a8eb8d",
      "metadata": {
        "id": "47a8eb8d"
      },
      "source": [
        "### Instantiate the loss to cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "a49ae3c9",
      "metadata": {
        "id": "a49ae3c9"
      },
      "outputs": [],
      "source": [
        "# use the cross-entropy loss\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a49045a",
      "metadata": {
        "id": "8a49045a"
      },
      "source": [
        "### Instantiate the optimizer to SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "aa3a04dd",
      "metadata": {
        "id": "aa3a04dd"
      },
      "outputs": [],
      "source": [
        "# use SGD .. use the momentum and weight decay vars\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93ef11d",
      "metadata": {
        "id": "f93ef11d"
      },
      "source": [
        "#### Create the learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "a0e1727a",
      "metadata": {
        "id": "a0e1727a"
      },
      "outputs": [],
      "source": [
        "# use CosineAnnealingLR\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "2fe08caa",
      "metadata": {
        "id": "2fe08caa"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean_RGB, cifar_std_RGB),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "236528e1",
      "metadata": {
        "id": "236528e1"
      },
      "source": [
        "### Create the train dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "7c29f6b1",
      "metadata": {
        "id": "7c29f6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccefb185-e778-4802-ff06-f90e9bb337b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# use torchvision.datasets.CIFAR10\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "63dfe3c0",
      "metadata": {
        "id": "63dfe3c0"
      },
      "outputs": [],
      "source": [
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean_RGB, cifar_std_RGB),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ca6c39",
      "metadata": {
        "id": "38ca6c39"
      },
      "source": [
        "### Create the val dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "42d58f82",
      "metadata": {
        "id": "42d58f82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a26671c-867b-49cd-97db-e35ce056ab55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# use torchvision.datasets.CIFAR10\n",
        "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a291660",
      "metadata": {
        "id": "3a291660"
      },
      "source": [
        "### Create the train dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "574373be",
      "metadata": {
        "id": "574373be"
      },
      "outputs": [],
      "source": [
        "# fill this in\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH, shuffle=True, num_workers=WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b280c6e0",
      "metadata": {
        "id": "b280c6e0"
      },
      "source": [
        "### Create the c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "6aa623fe",
      "metadata": {
        "id": "6aa623fe"
      },
      "outputs": [],
      "source": [
        "# fill this in..\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=VAL_BATCH, shuffle=False, num_workers=WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "7cfa6766",
      "metadata": {
        "id": "7cfa6766"
      },
      "outputs": [],
      "source": [
        "best_acc1 = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "0d0620a6",
      "metadata": {
        "id": "0d0620a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24f755e-f307-47b3-9549-2db9edd53b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc8c5eea550>Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc8c5eea550>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "        self._shutdown_workers()self._shutdown_workers()\n",
            "\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "        if w.is_alive():if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionError: AssertionErrorcan only test a child process\n",
            ": can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  0/391]\tTime  0.755 ( 0.755)\tData  0.711 ( 0.711)\tLoss 9.3811e-01 (9.3811e-01)\tAcc@1  66.41 ( 66.41)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [0][ 50/391]\tTime  0.185 ( 0.095)\tData  0.103 ( 0.054)\tLoss 7.8098e-01 (8.4166e-01)\tAcc@1  71.09 ( 70.48)\tAcc@5  98.44 ( 98.09)\n",
            "Epoch: [0][100/391]\tTime  0.046 ( 0.074)\tData  0.011 ( 0.037)\tLoss 9.4617e-01 (8.3165e-01)\tAcc@1  68.75 ( 70.98)\tAcc@5  98.44 ( 98.00)\n",
            "Epoch: [0][150/391]\tTime  0.040 ( 0.068)\tData  0.007 ( 0.031)\tLoss 9.8807e-01 (8.3800e-01)\tAcc@1  63.28 ( 70.75)\tAcc@5  97.66 ( 97.84)\n",
            "Epoch: [0][200/391]\tTime  0.042 ( 0.064)\tData  0.008 ( 0.028)\tLoss 1.0130e+00 (8.4537e-01)\tAcc@1  64.84 ( 70.57)\tAcc@5  97.66 ( 97.79)\n",
            "Epoch: [0][250/391]\tTime  0.031 ( 0.067)\tData  0.001 ( 0.030)\tLoss 6.8106e-01 (8.4047e-01)\tAcc@1  74.22 ( 70.67)\tAcc@5  99.22 ( 97.82)\n",
            "Epoch: [0][300/391]\tTime  0.056 ( 0.064)\tData  0.019 ( 0.028)\tLoss 7.5024e-01 (8.4501e-01)\tAcc@1  74.22 ( 70.52)\tAcc@5  99.22 ( 97.79)\n",
            "Epoch: [0][350/391]\tTime  0.066 ( 0.063)\tData  0.037 ( 0.026)\tLoss 8.5570e-01 (8.4400e-01)\tAcc@1  70.31 ( 70.50)\tAcc@5  99.22 ( 97.80)\n",
            "Test: [ 0/79]\tTime  0.159 ( 0.159)\tLoss 8.0477e-01 (8.0477e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  97.66 ( 97.66)\n",
            "Test: [50/79]\tTime  0.016 ( 0.032)\tLoss 9.4479e-01 (8.9072e-01)\tAcc@1  66.41 ( 68.86)\tAcc@5  97.66 ( 97.61)\n",
            " * Acc@1 68.850 Acc@5 97.560\n",
            "lr: [0.03455569536622452]\n",
            "Epoch: [1][  0/391]\tTime  0.244 ( 0.244)\tData  0.199 ( 0.199)\tLoss 7.9872e-01 (7.9872e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [1][ 50/391]\tTime  0.064 ( 0.065)\tData  0.031 ( 0.029)\tLoss 7.9536e-01 (8.0661e-01)\tAcc@1  71.88 ( 71.40)\tAcc@5  98.44 ( 98.16)\n",
            "Epoch: [1][100/391]\tTime  0.051 ( 0.058)\tData  0.018 ( 0.024)\tLoss 9.0919e-01 (8.0415e-01)\tAcc@1  69.53 ( 71.81)\tAcc@5  98.44 ( 97.88)\n",
            "Epoch: [1][150/391]\tTime  0.065 ( 0.057)\tData  0.037 ( 0.024)\tLoss 7.1505e-01 (8.0686e-01)\tAcc@1  74.22 ( 71.66)\tAcc@5  99.22 ( 97.96)\n",
            "Epoch: [1][200/391]\tTime  0.048 ( 0.056)\tData  0.001 ( 0.023)\tLoss 7.0664e-01 (8.0511e-01)\tAcc@1  73.44 ( 71.85)\tAcc@5  99.22 ( 97.95)\n",
            "Epoch: [1][250/391]\tTime  0.037 ( 0.059)\tData  0.001 ( 0.025)\tLoss 6.6814e-01 (8.0665e-01)\tAcc@1  76.56 ( 71.80)\tAcc@5  99.22 ( 97.93)\n",
            "Epoch: [1][300/391]\tTime  0.038 ( 0.057)\tData  0.002 ( 0.024)\tLoss 6.3793e-01 (8.0542e-01)\tAcc@1  75.78 ( 71.92)\tAcc@5  98.44 ( 97.92)\n",
            "Epoch: [1][350/391]\tTime  0.038 ( 0.057)\tData  0.004 ( 0.024)\tLoss 8.8210e-01 (8.0325e-01)\tAcc@1  67.97 ( 71.95)\tAcc@5  96.09 ( 97.92)\n",
            "Test: [ 0/79]\tTime  0.161 ( 0.161)\tLoss 7.1741e-01 (7.1741e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.018 ( 0.031)\tLoss 9.1920e-01 (8.6427e-01)\tAcc@1  68.75 ( 70.31)\tAcc@5  96.88 ( 97.72)\n",
            " * Acc@1 70.540 Acc@5 97.780\n",
            "lr: [0.027307744965521366]\n",
            "Epoch: [2][  0/391]\tTime  0.287 ( 0.287)\tData  0.239 ( 0.239)\tLoss 8.1501e-01 (8.1501e-01)\tAcc@1  71.09 ( 71.09)\tAcc@5  97.66 ( 97.66)\n",
            "Epoch: [2][ 50/391]\tTime  0.041 ( 0.066)\tData  0.008 ( 0.029)\tLoss 6.9955e-01 (7.7763e-01)\tAcc@1  74.22 ( 72.64)\tAcc@5  97.66 ( 98.13)\n",
            "Epoch: [2][100/391]\tTime  0.096 ( 0.060)\tData  0.056 ( 0.024)\tLoss 8.0681e-01 (7.7557e-01)\tAcc@1  72.66 ( 73.00)\tAcc@5  96.09 ( 98.09)\n",
            "Epoch: [2][150/391]\tTime  0.043 ( 0.058)\tData  0.007 ( 0.023)\tLoss 7.6001e-01 (7.6971e-01)\tAcc@1  74.22 ( 73.18)\tAcc@5  96.88 ( 98.10)\n",
            "Epoch: [2][200/391]\tTime  0.071 ( 0.057)\tData  0.043 ( 0.022)\tLoss 8.0099e-01 (7.6560e-01)\tAcc@1  73.44 ( 73.35)\tAcc@5  98.44 ( 98.08)\n",
            "Epoch: [2][250/391]\tTime  0.025 ( 0.060)\tData  0.002 ( 0.024)\tLoss 6.6378e-01 (7.6157e-01)\tAcc@1  72.66 ( 73.43)\tAcc@5  98.44 ( 98.11)\n",
            "Epoch: [2][300/391]\tTime  0.031 ( 0.058)\tData  0.005 ( 0.024)\tLoss 8.1286e-01 (7.5955e-01)\tAcc@1  64.06 ( 73.49)\tAcc@5  98.44 ( 98.16)\n",
            "Epoch: [2][350/391]\tTime  0.049 ( 0.058)\tData  0.002 ( 0.023)\tLoss 6.9909e-01 (7.5691e-01)\tAcc@1  76.56 ( 73.58)\tAcc@5  98.44 ( 98.18)\n",
            "Test: [ 0/79]\tTime  0.183 ( 0.183)\tLoss 7.4224e-01 (7.4224e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  97.66 ( 97.66)\n",
            "Test: [50/79]\tTime  0.032 ( 0.033)\tLoss 7.9967e-01 (7.7520e-01)\tAcc@1  74.22 ( 73.15)\tAcc@5  97.66 ( 98.38)\n",
            " * Acc@1 72.960 Acc@5 98.420\n",
            "lr: [0.020618676311637815]\n",
            "Epoch: [3][  0/391]\tTime  0.383 ( 0.383)\tData  0.332 ( 0.332)\tLoss 7.2301e-01 (7.2301e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [3][ 50/391]\tTime  0.064 ( 0.064)\tData  0.034 ( 0.030)\tLoss 7.7347e-01 (6.8872e-01)\tAcc@1  73.44 ( 75.83)\tAcc@5  96.88 ( 98.70)\n",
            "Epoch: [3][100/391]\tTime  0.046 ( 0.059)\tData  0.007 ( 0.025)\tLoss 5.5211e-01 (6.8431e-01)\tAcc@1  81.25 ( 76.21)\tAcc@5  97.66 ( 98.58)\n",
            "Epoch: [3][150/391]\tTime  0.081 ( 0.057)\tData  0.045 ( 0.023)\tLoss 7.7911e-01 (6.9650e-01)\tAcc@1  68.75 ( 75.68)\tAcc@5  97.66 ( 98.56)\n",
            "Epoch: [3][200/391]\tTime  0.075 ( 0.057)\tData  0.052 ( 0.024)\tLoss 6.7911e-01 (6.9816e-01)\tAcc@1  74.22 ( 75.53)\tAcc@5  98.44 ( 98.55)\n",
            "Epoch: [3][250/391]\tTime  0.083 ( 0.060)\tData  0.050 ( 0.026)\tLoss 7.6455e-01 (6.9828e-01)\tAcc@1  77.34 ( 75.54)\tAcc@5  96.09 ( 98.57)\n",
            "Epoch: [3][300/391]\tTime  0.051 ( 0.059)\tData  0.028 ( 0.025)\tLoss 5.3516e-01 (6.9836e-01)\tAcc@1  81.25 ( 75.53)\tAcc@5  99.22 ( 98.56)\n",
            "Epoch: [3][350/391]\tTime  0.059 ( 0.058)\tData  0.004 ( 0.024)\tLoss 6.9105e-01 (7.0115e-01)\tAcc@1  74.22 ( 75.44)\tAcc@5  98.44 ( 98.54)\n",
            "Test: [ 0/79]\tTime  0.169 ( 0.169)\tLoss 5.9305e-01 (5.9305e-01)\tAcc@1  81.25 ( 81.25)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.063 ( 0.039)\tLoss 7.7231e-01 (7.1484e-01)\tAcc@1  75.78 ( 75.41)\tAcc@5  98.44 ( 98.42)\n",
            " * Acc@1 75.370 Acc@5 98.510\n",
            "lr: [0.014653196474578562]\n",
            "Epoch: [4][  0/391]\tTime  0.320 ( 0.320)\tData  0.267 ( 0.267)\tLoss 7.6088e-01 (7.6088e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [4][ 50/391]\tTime  0.042 ( 0.058)\tData  0.001 ( 0.023)\tLoss 7.1712e-01 (6.9041e-01)\tAcc@1  73.44 ( 75.81)\tAcc@5  98.44 ( 98.47)\n",
            "Epoch: [4][100/391]\tTime  0.064 ( 0.056)\tData  0.042 ( 0.020)\tLoss 6.8806e-01 (6.7426e-01)\tAcc@1  76.56 ( 76.25)\tAcc@5  96.88 ( 98.59)\n",
            "Epoch: [4][150/391]\tTime  0.044 ( 0.054)\tData  0.008 ( 0.020)\tLoss 7.0025e-01 (6.6245e-01)\tAcc@1  72.66 ( 76.61)\tAcc@5  97.66 ( 98.69)\n",
            "Epoch: [4][200/391]\tTime  0.040 ( 0.055)\tData  0.001 ( 0.021)\tLoss 6.2677e-01 (6.5845e-01)\tAcc@1  79.69 ( 76.80)\tAcc@5  97.66 ( 98.70)\n",
            "Epoch: [4][250/391]\tTime  0.045 ( 0.057)\tData  0.015 ( 0.023)\tLoss 5.2800e-01 (6.6354e-01)\tAcc@1  79.69 ( 76.75)\tAcc@5 100.00 ( 98.68)\n",
            "Epoch: [4][300/391]\tTime  0.073 ( 0.057)\tData  0.040 ( 0.023)\tLoss 7.8730e-01 (6.6149e-01)\tAcc@1  67.19 ( 76.72)\tAcc@5  99.22 ( 98.69)\n",
            "Epoch: [4][350/391]\tTime  0.083 ( 0.056)\tData  0.050 ( 0.022)\tLoss 7.0518e-01 (6.5982e-01)\tAcc@1  73.44 ( 76.85)\tAcc@5  97.66 ( 98.68)\n",
            "Test: [ 0/79]\tTime  0.161 ( 0.161)\tLoss 5.6053e-01 (5.6053e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  98.44 ( 98.44)\n",
            "Test: [50/79]\tTime  0.035 ( 0.041)\tLoss 6.5831e-01 (6.9450e-01)\tAcc@1  72.66 ( 76.07)\tAcc@5  99.22 ( 98.76)\n",
            " * Acc@1 76.340 Acc@5 98.710\n",
            "lr: [0.00955819536622451]\n",
            "Epoch: [5][  0/391]\tTime  0.241 ( 0.241)\tData  0.197 ( 0.197)\tLoss 8.4100e-01 (8.4100e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  97.66 ( 97.66)\n",
            "Epoch: [5][ 50/391]\tTime  0.049 ( 0.055)\tData  0.012 ( 0.020)\tLoss 4.8999e-01 (6.3268e-01)\tAcc@1  81.25 ( 77.59)\tAcc@5  99.22 ( 98.70)\n",
            "Epoch: [5][100/391]\tTime  0.043 ( 0.054)\tData  0.008 ( 0.019)\tLoss 6.5150e-01 (6.2133e-01)\tAcc@1  78.12 ( 77.99)\tAcc@5  97.66 ( 98.77)\n",
            "Epoch: [5][150/391]\tTime  0.062 ( 0.054)\tData  0.029 ( 0.019)\tLoss 7.2983e-01 (6.1470e-01)\tAcc@1  73.44 ( 78.41)\tAcc@5  96.88 ( 98.79)\n",
            "Epoch: [5][200/391]\tTime  0.043 ( 0.056)\tData  0.008 ( 0.021)\tLoss 5.5405e-01 (6.0845e-01)\tAcc@1  78.12 ( 78.70)\tAcc@5 100.00 ( 98.87)\n",
            "Epoch: [5][250/391]\tTime  0.027 ( 0.057)\tData  0.001 ( 0.024)\tLoss 6.9266e-01 (6.1311e-01)\tAcc@1  78.12 ( 78.64)\tAcc@5  98.44 ( 98.83)\n",
            "Epoch: [5][300/391]\tTime  0.031 ( 0.056)\tData  0.002 ( 0.023)\tLoss 4.9106e-01 (6.1465e-01)\tAcc@1  81.25 ( 78.49)\tAcc@5 100.00 ( 98.84)\n",
            "Epoch: [5][350/391]\tTime  0.036 ( 0.056)\tData  0.002 ( 0.023)\tLoss 7.3056e-01 (6.1409e-01)\tAcc@1  78.91 ( 78.50)\tAcc@5  99.22 ( 98.84)\n",
            "Test: [ 0/79]\tTime  0.158 ( 0.158)\tLoss 5.4678e-01 (5.4678e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.018 ( 0.036)\tLoss 6.9581e-01 (6.3759e-01)\tAcc@1  76.56 ( 78.00)\tAcc@5  97.66 ( 98.77)\n",
            " * Acc@1 78.160 Acc@5 98.750\n",
            "lr: [0.005459128823202554]\n",
            "Epoch: [6][  0/391]\tTime  0.333 ( 0.333)\tData  0.285 ( 0.285)\tLoss 6.1597e-01 (6.1597e-01)\tAcc@1  78.12 ( 78.12)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [6][ 50/391]\tTime  0.031 ( 0.058)\tData  0.002 ( 0.023)\tLoss 5.1850e-01 (5.8151e-01)\tAcc@1  84.38 ( 80.04)\tAcc@5 100.00 ( 98.91)\n",
            "Epoch: [6][100/391]\tTime  0.064 ( 0.055)\tData  0.033 ( 0.022)\tLoss 6.0437e-01 (5.7959e-01)\tAcc@1  78.91 ( 79.83)\tAcc@5  97.66 ( 98.96)\n",
            "Epoch: [6][150/391]\tTime  0.070 ( 0.054)\tData  0.036 ( 0.021)\tLoss 5.2881e-01 (5.7863e-01)\tAcc@1  80.47 ( 79.87)\tAcc@5  99.22 ( 98.97)\n",
            "Epoch: [6][200/391]\tTime  0.069 ( 0.054)\tData  0.013 ( 0.021)\tLoss 6.0086e-01 (5.7650e-01)\tAcc@1  77.34 ( 79.94)\tAcc@5  97.66 ( 98.95)\n",
            "Epoch: [6][250/391]\tTime  0.043 ( 0.057)\tData  0.008 ( 0.023)\tLoss 5.3296e-01 (5.7338e-01)\tAcc@1  82.81 ( 79.96)\tAcc@5 100.00 ( 98.99)\n",
            "Epoch: [6][300/391]\tTime  0.061 ( 0.056)\tData  0.023 ( 0.022)\tLoss 5.2088e-01 (5.7112e-01)\tAcc@1  81.25 ( 79.95)\tAcc@5 100.00 ( 99.02)\n",
            "Epoch: [6][350/391]\tTime  0.081 ( 0.056)\tData  0.046 ( 0.022)\tLoss 6.3334e-01 (5.7572e-01)\tAcc@1  81.25 ( 79.79)\tAcc@5  96.88 ( 99.00)\n",
            "Test: [ 0/79]\tTime  0.156 ( 0.156)\tLoss 6.1994e-01 (6.1994e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  98.44 ( 98.44)\n",
            "Test: [50/79]\tTime  0.016 ( 0.031)\tLoss 5.9772e-01 (6.1754e-01)\tAcc@1  79.69 ( 78.92)\tAcc@5  99.22 ( 98.81)\n",
            " * Acc@1 78.860 Acc@5 98.840\n",
            "lr: [0.0024569294678238]\n",
            "Epoch: [7][  0/391]\tTime  0.404 ( 0.404)\tData  0.317 ( 0.317)\tLoss 6.6063e-01 (6.6063e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [7][ 50/391]\tTime  0.043 ( 0.061)\tData  0.003 ( 0.026)\tLoss 5.5747e-01 (5.5161e-01)\tAcc@1  78.12 ( 80.16)\tAcc@5  99.22 ( 99.17)\n",
            "Epoch: [7][100/391]\tTime  0.053 ( 0.056)\tData  0.027 ( 0.023)\tLoss 5.8429e-01 (5.4009e-01)\tAcc@1  83.59 ( 81.01)\tAcc@5  97.66 ( 99.12)\n",
            "Epoch: [7][150/391]\tTime  0.094 ( 0.055)\tData  0.053 ( 0.021)\tLoss 6.9465e-01 (5.4563e-01)\tAcc@1  75.00 ( 80.71)\tAcc@5  98.44 ( 99.19)\n",
            "Epoch: [7][200/391]\tTime  0.101 ( 0.055)\tData  0.068 ( 0.022)\tLoss 4.7856e-01 (5.4201e-01)\tAcc@1  85.16 ( 81.00)\tAcc@5  98.44 ( 99.19)\n",
            "Epoch: [7][250/391]\tTime  0.065 ( 0.058)\tData  0.035 ( 0.024)\tLoss 5.1143e-01 (5.4378e-01)\tAcc@1  78.91 ( 80.88)\tAcc@5 100.00 ( 99.19)\n",
            "Epoch: [7][300/391]\tTime  0.032 ( 0.057)\tData  0.003 ( 0.024)\tLoss 3.7168e-01 (5.4237e-01)\tAcc@1  87.50 ( 80.99)\tAcc@5 100.00 ( 99.20)\n",
            "Epoch: [7][350/391]\tTime  0.042 ( 0.056)\tData  0.007 ( 0.023)\tLoss 5.3260e-01 (5.4320e-01)\tAcc@1  82.81 ( 80.95)\tAcc@5  99.22 ( 99.17)\n",
            "Test: [ 0/79]\tTime  0.173 ( 0.173)\tLoss 5.3460e-01 (5.3460e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.008 ( 0.032)\tLoss 6.1033e-01 (5.8914e-01)\tAcc@1  78.91 ( 80.15)\tAcc@5  98.44 ( 98.97)\n",
            " * Acc@1 80.050 Acc@5 98.960\n",
            "lr: [0.000625521411946093]\n",
            "Epoch: [8][  0/391]\tTime  0.370 ( 0.370)\tData  0.307 ( 0.307)\tLoss 4.9292e-01 (4.9292e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [8][ 50/391]\tTime  0.039 ( 0.061)\tData  0.005 ( 0.026)\tLoss 6.9765e-01 (5.5472e-01)\tAcc@1  72.66 ( 80.47)\tAcc@5  99.22 ( 99.00)\n",
            "Epoch: [8][100/391]\tTime  0.046 ( 0.056)\tData  0.009 ( 0.022)\tLoss 4.7749e-01 (5.4199e-01)\tAcc@1  84.38 ( 80.89)\tAcc@5  98.44 ( 99.07)\n",
            "Epoch: [8][150/391]\tTime  0.034 ( 0.054)\tData  0.001 ( 0.022)\tLoss 6.0146e-01 (5.3667e-01)\tAcc@1  79.69 ( 81.13)\tAcc@5  98.44 ( 99.15)\n",
            "Epoch: [8][200/391]\tTime  0.055 ( 0.054)\tData  0.025 ( 0.022)\tLoss 7.5026e-01 (5.3407e-01)\tAcc@1  74.22 ( 81.21)\tAcc@5  99.22 ( 99.15)\n",
            "Epoch: [8][250/391]\tTime  0.065 ( 0.058)\tData  0.029 ( 0.024)\tLoss 5.8392e-01 (5.2813e-01)\tAcc@1  78.91 ( 81.40)\tAcc@5  98.44 ( 99.16)\n",
            "Epoch: [8][300/391]\tTime  0.037 ( 0.057)\tData  0.006 ( 0.023)\tLoss 4.3420e-01 (5.2311e-01)\tAcc@1  85.94 ( 81.60)\tAcc@5 100.00 ( 99.19)\n",
            "Epoch: [8][350/391]\tTime  0.050 ( 0.056)\tData  0.022 ( 0.023)\tLoss 6.2711e-01 (5.2574e-01)\tAcc@1  76.56 ( 81.47)\tAcc@5  98.44 ( 99.15)\n",
            "Test: [ 0/79]\tTime  0.167 ( 0.167)\tLoss 5.3159e-01 (5.3159e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.018 ( 0.033)\tLoss 5.7986e-01 (5.7647e-01)\tAcc@1  81.25 ( 80.58)\tAcc@5  99.22 ( 99.08)\n",
            " * Acc@1 80.550 Acc@5 99.070\n",
            "lr: [1e-05]\n",
            "Epoch: [9][  0/391]\tTime  0.398 ( 0.398)\tData  0.337 ( 0.337)\tLoss 5.3427e-01 (5.3427e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [9][ 50/391]\tTime  0.044 ( 0.063)\tData  0.007 ( 0.027)\tLoss 5.0333e-01 (5.1548e-01)\tAcc@1  81.25 ( 82.09)\tAcc@5  99.22 ( 99.19)\n",
            "Epoch: [9][100/391]\tTime  0.076 ( 0.057)\tData  0.046 ( 0.022)\tLoss 4.7946e-01 (5.2421e-01)\tAcc@1  82.03 ( 81.90)\tAcc@5 100.00 ( 99.18)\n",
            "Epoch: [9][150/391]\tTime  0.037 ( 0.055)\tData  0.004 ( 0.021)\tLoss 4.6778e-01 (5.2315e-01)\tAcc@1  83.59 ( 81.82)\tAcc@5 100.00 ( 99.20)\n",
            "Epoch: [9][200/391]\tTime  0.037 ( 0.054)\tData  0.001 ( 0.021)\tLoss 6.2688e-01 (5.1832e-01)\tAcc@1  78.12 ( 81.81)\tAcc@5 100.00 ( 99.23)\n",
            "Epoch: [9][250/391]\tTime  0.036 ( 0.057)\tData  0.006 ( 0.023)\tLoss 6.7086e-01 (5.1948e-01)\tAcc@1  76.56 ( 81.77)\tAcc@5  98.44 ( 99.23)\n",
            "Epoch: [9][300/391]\tTime  0.040 ( 0.056)\tData  0.001 ( 0.023)\tLoss 3.5221e-01 (5.1946e-01)\tAcc@1  89.06 ( 81.78)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [9][350/391]\tTime  0.033 ( 0.056)\tData  0.001 ( 0.022)\tLoss 5.7086e-01 (5.1753e-01)\tAcc@1  81.25 ( 81.87)\tAcc@5  98.44 ( 99.23)\n",
            "Test: [ 0/79]\tTime  0.158 ( 0.158)\tLoss 5.3584e-01 (5.3584e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.019 ( 0.033)\tLoss 5.8441e-01 (5.7956e-01)\tAcc@1  81.25 ( 80.44)\tAcc@5  98.44 ( 99.08)\n",
            " * Acc@1 80.390 Acc@5 99.080\n",
            "lr: [0.0006255214119460872]\n",
            "Epoch: [10][  0/391]\tTime  0.364 ( 0.364)\tData  0.313 ( 0.313)\tLoss 4.1004e-01 (4.1004e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [10][ 50/391]\tTime  0.029 ( 0.068)\tData  0.004 ( 0.031)\tLoss 4.8150e-01 (5.2967e-01)\tAcc@1  79.69 ( 81.53)\tAcc@5 100.00 ( 99.07)\n",
            "Epoch: [10][100/391]\tTime  0.036 ( 0.060)\tData  0.001 ( 0.025)\tLoss 6.5038e-01 (5.2981e-01)\tAcc@1  78.91 ( 81.55)\tAcc@5  99.22 ( 99.03)\n",
            "Epoch: [10][150/391]\tTime  0.038 ( 0.058)\tData  0.006 ( 0.023)\tLoss 4.1987e-01 (5.3060e-01)\tAcc@1  84.38 ( 81.53)\tAcc@5  98.44 ( 99.03)\n",
            "Epoch: [10][200/391]\tTime  0.066 ( 0.056)\tData  0.039 ( 0.022)\tLoss 4.7398e-01 (5.2609e-01)\tAcc@1  84.38 ( 81.69)\tAcc@5  99.22 ( 99.08)\n",
            "Epoch: [10][250/391]\tTime  0.037 ( 0.059)\tData  0.001 ( 0.025)\tLoss 7.3249e-01 (5.2597e-01)\tAcc@1  80.47 ( 81.70)\tAcc@5  98.44 ( 99.09)\n",
            "Epoch: [10][300/391]\tTime  0.048 ( 0.058)\tData  0.026 ( 0.024)\tLoss 6.3508e-01 (5.2361e-01)\tAcc@1  78.91 ( 81.75)\tAcc@5  99.22 ( 99.12)\n",
            "Epoch: [10][350/391]\tTime  0.036 ( 0.057)\tData  0.013 ( 0.023)\tLoss 4.2816e-01 (5.2339e-01)\tAcc@1  84.38 ( 81.72)\tAcc@5 100.00 ( 99.13)\n",
            "Test: [ 0/79]\tTime  0.164 ( 0.164)\tLoss 5.3362e-01 (5.3362e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  98.44 ( 98.44)\n",
            "Test: [50/79]\tTime  0.023 ( 0.032)\tLoss 5.8435e-01 (5.7736e-01)\tAcc@1  78.91 ( 80.36)\tAcc@5  98.44 ( 99.02)\n",
            " * Acc@1 80.330 Acc@5 99.050\n",
            "lr: [0.0024569294678237884]\n",
            "Epoch: [11][  0/391]\tTime  0.405 ( 0.405)\tData  0.355 ( 0.355)\tLoss 5.1262e-01 (5.1262e-01)\tAcc@1  78.91 ( 78.91)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [11][ 50/391]\tTime  0.072 ( 0.070)\tData  0.037 ( 0.033)\tLoss 4.7569e-01 (5.2779e-01)\tAcc@1  82.03 ( 81.59)\tAcc@5 100.00 ( 99.16)\n",
            "Epoch: [11][100/391]\tTime  0.026 ( 0.061)\tData  0.003 ( 0.026)\tLoss 5.1498e-01 (5.3159e-01)\tAcc@1  77.34 ( 81.31)\tAcc@5 100.00 ( 99.13)\n",
            "Epoch: [11][150/391]\tTime  0.042 ( 0.058)\tData  0.001 ( 0.023)\tLoss 5.5155e-01 (5.2684e-01)\tAcc@1  77.34 ( 81.53)\tAcc@5  99.22 ( 99.17)\n",
            "Epoch: [11][200/391]\tTime  0.069 ( 0.057)\tData  0.047 ( 0.022)\tLoss 5.3503e-01 (5.2629e-01)\tAcc@1  77.34 ( 81.50)\tAcc@5  99.22 ( 99.20)\n",
            "Epoch: [11][250/391]\tTime  0.056 ( 0.059)\tData  0.034 ( 0.025)\tLoss 6.0441e-01 (5.2981e-01)\tAcc@1  78.12 ( 81.42)\tAcc@5  99.22 ( 99.19)\n",
            "Epoch: [11][300/391]\tTime  0.071 ( 0.058)\tData  0.043 ( 0.024)\tLoss 4.1842e-01 (5.3116e-01)\tAcc@1  85.16 ( 81.37)\tAcc@5  99.22 ( 99.16)\n",
            "Epoch: [11][350/391]\tTime  0.025 ( 0.057)\tData  0.001 ( 0.024)\tLoss 5.4408e-01 (5.3319e-01)\tAcc@1  81.25 ( 81.26)\tAcc@5  99.22 ( 99.15)\n",
            "Test: [ 0/79]\tTime  0.153 ( 0.153)\tLoss 5.4963e-01 (5.4963e-01)\tAcc@1  82.81 ( 82.81)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.038 ( 0.032)\tLoss 6.2858e-01 (5.7886e-01)\tAcc@1  78.12 ( 80.24)\tAcc@5  99.22 ( 99.02)\n",
            " * Acc@1 80.110 Acc@5 99.040\n",
            "lr: [0.005459128823202547]\n",
            "Epoch: [12][  0/391]\tTime  0.372 ( 0.372)\tData  0.305 ( 0.305)\tLoss 3.5497e-01 (3.5497e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [12][ 50/391]\tTime  0.037 ( 0.067)\tData  0.004 ( 0.028)\tLoss 5.8262e-01 (5.1568e-01)\tAcc@1  79.69 ( 81.86)\tAcc@5  98.44 ( 99.23)\n",
            "Epoch: [12][100/391]\tTime  0.066 ( 0.060)\tData  0.036 ( 0.023)\tLoss 6.8293e-01 (5.2698e-01)\tAcc@1  82.03 ( 81.88)\tAcc@5  98.44 ( 99.11)\n",
            "Epoch: [12][150/391]\tTime  0.048 ( 0.057)\tData  0.026 ( 0.022)\tLoss 4.7850e-01 (5.3901e-01)\tAcc@1  80.47 ( 81.32)\tAcc@5 100.00 ( 99.10)\n",
            "Epoch: [12][200/391]\tTime  0.046 ( 0.056)\tData  0.017 ( 0.021)\tLoss 6.5384e-01 (5.3939e-01)\tAcc@1  79.69 ( 81.19)\tAcc@5  99.22 ( 99.11)\n",
            "Epoch: [12][250/391]\tTime  0.070 ( 0.059)\tData  0.039 ( 0.024)\tLoss 6.2801e-01 (5.4077e-01)\tAcc@1  77.34 ( 81.10)\tAcc@5  97.66 ( 99.12)\n",
            "Epoch: [12][300/391]\tTime  0.043 ( 0.058)\tData  0.007 ( 0.023)\tLoss 5.9469e-01 (5.4116e-01)\tAcc@1  79.69 ( 81.06)\tAcc@5  99.22 ( 99.11)\n",
            "Epoch: [12][350/391]\tTime  0.056 ( 0.057)\tData  0.026 ( 0.023)\tLoss 6.3792e-01 (5.4239e-01)\tAcc@1  78.91 ( 80.97)\tAcc@5  99.22 ( 99.13)\n",
            "Test: [ 0/79]\tTime  0.165 ( 0.165)\tLoss 5.1948e-01 (5.1948e-01)\tAcc@1  82.81 ( 82.81)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.040 ( 0.032)\tLoss 6.5554e-01 (5.9182e-01)\tAcc@1  78.91 ( 79.81)\tAcc@5  99.22 ( 98.87)\n",
            " * Acc@1 79.670 Acc@5 98.970\n",
            "lr: [0.009558195366224497]\n",
            "Epoch: [13][  0/391]\tTime  0.362 ( 0.362)\tData  0.290 ( 0.290)\tLoss 4.2878e-01 (4.2878e-01)\tAcc@1  85.94 ( 85.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [13][ 50/391]\tTime  0.050 ( 0.068)\tData  0.010 ( 0.029)\tLoss 4.6862e-01 (5.5994e-01)\tAcc@1  84.38 ( 80.64)\tAcc@5  99.22 ( 99.00)\n",
            "Epoch: [13][100/391]\tTime  0.041 ( 0.059)\tData  0.006 ( 0.023)\tLoss 7.1665e-01 (5.7594e-01)\tAcc@1  68.75 ( 80.16)\tAcc@5  98.44 ( 98.92)\n",
            "Epoch: [13][150/391]\tTime  0.071 ( 0.057)\tData  0.039 ( 0.022)\tLoss 6.2469e-01 (5.7705e-01)\tAcc@1  78.91 ( 80.15)\tAcc@5  98.44 ( 98.87)\n",
            "Epoch: [13][200/391]\tTime  0.044 ( 0.056)\tData  0.008 ( 0.021)\tLoss 4.6003e-01 (5.7696e-01)\tAcc@1  84.38 ( 79.96)\tAcc@5 100.00 ( 98.89)\n",
            "Epoch: [13][250/391]\tTime  0.085 ( 0.058)\tData  0.041 ( 0.023)\tLoss 7.3684e-01 (5.8336e-01)\tAcc@1  75.00 ( 79.77)\tAcc@5  99.22 ( 98.89)\n",
            "Epoch: [13][300/391]\tTime  0.066 ( 0.057)\tData  0.033 ( 0.023)\tLoss 6.4628e-01 (5.8091e-01)\tAcc@1  78.12 ( 79.86)\tAcc@5  99.22 ( 98.90)\n",
            "Epoch: [13][350/391]\tTime  0.036 ( 0.057)\tData  0.001 ( 0.022)\tLoss 3.9509e-01 (5.7986e-01)\tAcc@1  85.94 ( 79.85)\tAcc@5 100.00 ( 98.93)\n",
            "Test: [ 0/79]\tTime  0.174 ( 0.174)\tLoss 5.6718e-01 (5.6718e-01)\tAcc@1  81.25 ( 81.25)\tAcc@5  97.66 ( 97.66)\n",
            "Test: [50/79]\tTime  0.014 ( 0.032)\tLoss 6.3644e-01 (6.2745e-01)\tAcc@1  80.47 ( 78.42)\tAcc@5 100.00 ( 98.82)\n",
            " * Acc@1 78.320 Acc@5 98.770\n",
            "lr: [0.014653196474578549]\n",
            "Epoch: [14][  0/391]\tTime  0.290 ( 0.290)\tData  0.217 ( 0.217)\tLoss 4.7990e-01 (4.7990e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [14][ 50/391]\tTime  0.037 ( 0.075)\tData  0.001 ( 0.036)\tLoss 7.8248e-01 (5.8769e-01)\tAcc@1  71.09 ( 79.49)\tAcc@5 100.00 ( 98.94)\n",
            "Epoch: [14][100/391]\tTime  0.035 ( 0.063)\tData  0.001 ( 0.027)\tLoss 5.5212e-01 (5.9300e-01)\tAcc@1  78.91 ( 79.46)\tAcc@5 100.00 ( 98.92)\n",
            "Epoch: [14][150/391]\tTime  0.065 ( 0.059)\tData  0.040 ( 0.024)\tLoss 4.9868e-01 (6.0213e-01)\tAcc@1  84.38 ( 79.09)\tAcc@5 100.00 ( 98.91)\n",
            "Epoch: [14][200/391]\tTime  0.049 ( 0.057)\tData  0.024 ( 0.022)\tLoss 6.4942e-01 (6.0350e-01)\tAcc@1  77.34 ( 79.11)\tAcc@5  98.44 ( 98.90)\n",
            "Epoch: [14][250/391]\tTime  0.168 ( 0.059)\tData  0.115 ( 0.024)\tLoss 7.2259e-01 (6.0743e-01)\tAcc@1  74.22 ( 78.83)\tAcc@5  96.09 ( 98.85)\n",
            "Epoch: [14][300/391]\tTime  0.041 ( 0.058)\tData  0.006 ( 0.024)\tLoss 5.5956e-01 (6.0721e-01)\tAcc@1  83.59 ( 78.80)\tAcc@5  99.22 ( 98.85)\n",
            "Epoch: [14][350/391]\tTime  0.050 ( 0.057)\tData  0.016 ( 0.023)\tLoss 6.4506e-01 (6.0849e-01)\tAcc@1  76.56 ( 78.66)\tAcc@5  99.22 ( 98.85)\n",
            "Test: [ 0/79]\tTime  0.169 ( 0.169)\tLoss 6.5061e-01 (6.5061e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  97.66 ( 97.66)\n",
            "Test: [50/79]\tTime  0.047 ( 0.033)\tLoss 6.8093e-01 (6.6489e-01)\tAcc@1  74.22 ( 77.22)\tAcc@5  99.22 ( 98.59)\n",
            " * Acc@1 77.010 Acc@5 98.600\n",
            "lr: [0.0206186763116378]\n",
            "Epoch: [15][  0/391]\tTime  0.252 ( 0.252)\tData  0.202 ( 0.202)\tLoss 5.1303e-01 (5.1303e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [15][ 50/391]\tTime  0.049 ( 0.077)\tData  0.017 ( 0.037)\tLoss 6.3746e-01 (6.2538e-01)\tAcc@1  81.25 ( 78.12)\tAcc@5  98.44 ( 98.93)\n",
            "Epoch: [15][100/391]\tTime  0.030 ( 0.064)\tData  0.008 ( 0.027)\tLoss 5.9529e-01 (6.3840e-01)\tAcc@1  80.47 ( 77.54)\tAcc@5 100.00 ( 98.82)\n",
            "Epoch: [15][150/391]\tTime  0.063 ( 0.060)\tData  0.040 ( 0.025)\tLoss 7.9889e-01 (6.4928e-01)\tAcc@1  67.19 ( 77.00)\tAcc@5  97.66 ( 98.83)\n",
            "Epoch: [15][200/391]\tTime  0.035 ( 0.057)\tData  0.006 ( 0.023)\tLoss 6.3949e-01 (6.5814e-01)\tAcc@1  82.03 ( 76.74)\tAcc@5  99.22 ( 98.75)\n",
            "Epoch: [15][250/391]\tTime  0.046 ( 0.058)\tData  0.005 ( 0.024)\tLoss 5.7918e-01 (6.5805e-01)\tAcc@1  78.91 ( 76.72)\tAcc@5  97.66 ( 98.79)\n",
            "Epoch: [15][300/391]\tTime  0.076 ( 0.059)\tData  0.054 ( 0.024)\tLoss 5.1842e-01 (6.5187e-01)\tAcc@1  83.59 ( 76.94)\tAcc@5  99.22 ( 98.80)\n",
            "Epoch: [15][350/391]\tTime  0.066 ( 0.058)\tData  0.043 ( 0.023)\tLoss 7.7645e-01 (6.5668e-01)\tAcc@1  70.31 ( 76.82)\tAcc@5  97.66 ( 98.74)\n",
            "Test: [ 0/79]\tTime  0.162 ( 0.162)\tLoss 6.3365e-01 (6.3365e-01)\tAcc@1  78.12 ( 78.12)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.009 ( 0.032)\tLoss 7.0077e-01 (6.8655e-01)\tAcc@1  78.12 ( 76.49)\tAcc@5  97.66 ( 98.56)\n",
            " * Acc@1 76.600 Acc@5 98.520\n",
            "lr: [0.02730774496552135]\n",
            "Epoch: [16][  0/391]\tTime  0.248 ( 0.248)\tData  0.207 ( 0.207)\tLoss 4.7576e-01 (4.7576e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [16][ 50/391]\tTime  0.040 ( 0.074)\tData  0.001 ( 0.034)\tLoss 6.1424e-01 (6.5181e-01)\tAcc@1  78.12 ( 77.53)\tAcc@5 100.00 ( 98.85)\n",
            "Epoch: [16][100/391]\tTime  0.049 ( 0.063)\tData  0.010 ( 0.029)\tLoss 5.9788e-01 (6.8412e-01)\tAcc@1  80.47 ( 76.12)\tAcc@5 100.00 ( 98.62)\n",
            "Epoch: [16][150/391]\tTime  0.038 ( 0.060)\tData  0.001 ( 0.025)\tLoss 7.3190e-01 (6.8011e-01)\tAcc@1  76.56 ( 76.23)\tAcc@5  96.09 ( 98.61)\n",
            "Epoch: [16][200/391]\tTime  0.062 ( 0.058)\tData  0.032 ( 0.024)\tLoss 8.7080e-01 (6.8529e-01)\tAcc@1  70.31 ( 76.14)\tAcc@5  96.88 ( 98.55)\n",
            "Epoch: [16][250/391]\tTime  0.052 ( 0.057)\tData  0.012 ( 0.023)\tLoss 6.4247e-01 (6.8957e-01)\tAcc@1  75.78 ( 75.99)\tAcc@5  98.44 ( 98.50)\n",
            "Epoch: [16][300/391]\tTime  0.072 ( 0.059)\tData  0.044 ( 0.025)\tLoss 6.7081e-01 (6.8886e-01)\tAcc@1  74.22 ( 75.99)\tAcc@5 100.00 ( 98.54)\n",
            "Epoch: [16][350/391]\tTime  0.058 ( 0.058)\tData  0.033 ( 0.024)\tLoss 6.9688e-01 (6.8850e-01)\tAcc@1  75.00 ( 75.99)\tAcc@5  98.44 ( 98.51)\n",
            "Test: [ 0/79]\tTime  0.157 ( 0.157)\tLoss 7.9262e-01 (7.9262e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [50/79]\tTime  0.045 ( 0.031)\tLoss 8.5280e-01 (7.3011e-01)\tAcc@1  72.66 ( 75.21)\tAcc@5  96.88 ( 98.13)\n",
            " * Acc@1 74.870 Acc@5 98.220\n",
            "lr: [0.0345556953662245]\n",
            "Epoch: [17][  0/391]\tTime  0.242 ( 0.242)\tData  0.191 ( 0.191)\tLoss 6.3167e-01 (6.3167e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [17][ 50/391]\tTime  0.041 ( 0.069)\tData  0.005 ( 0.034)\tLoss 8.2426e-01 (6.9899e-01)\tAcc@1  69.53 ( 76.16)\tAcc@5  99.22 ( 98.53)\n",
            "Epoch: [17][100/391]\tTime  0.039 ( 0.063)\tData  0.001 ( 0.032)\tLoss 6.8320e-01 (7.0606e-01)\tAcc@1  74.22 ( 75.75)\tAcc@5 100.00 ( 98.55)\n",
            "Epoch: [17][150/391]\tTime  0.033 ( 0.059)\tData  0.001 ( 0.028)\tLoss 8.0216e-01 (7.1522e-01)\tAcc@1  69.53 ( 75.35)\tAcc@5  96.88 ( 98.45)\n",
            "Epoch: [17][200/391]\tTime  0.166 ( 0.059)\tData  0.124 ( 0.027)\tLoss 7.9627e-01 (7.1523e-01)\tAcc@1  71.88 ( 75.42)\tAcc@5  97.66 ( 98.41)\n",
            "Epoch: [17][250/391]\tTime  0.125 ( 0.063)\tData  0.098 ( 0.030)\tLoss 8.1590e-01 (7.1586e-01)\tAcc@1  72.66 ( 75.33)\tAcc@5  96.09 ( 98.39)\n",
            "Epoch: [17][300/391]\tTime  0.033 ( 0.062)\tData  0.001 ( 0.029)\tLoss 8.6664e-01 (7.1113e-01)\tAcc@1  69.53 ( 75.37)\tAcc@5 100.00 ( 98.43)\n",
            "Epoch: [17][350/391]\tTime  0.039 ( 0.061)\tData  0.001 ( 0.028)\tLoss 6.5436e-01 (7.1266e-01)\tAcc@1  76.56 ( 75.28)\tAcc@5  98.44 ( 98.41)\n",
            "Test: [ 0/79]\tTime  0.168 ( 0.168)\tLoss 7.5220e-01 (7.5220e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  97.66 ( 97.66)\n",
            "Test: [50/79]\tTime  0.051 ( 0.033)\tLoss 7.5155e-01 (8.0318e-01)\tAcc@1  75.78 ( 72.90)\tAcc@5  96.88 ( 97.78)\n",
            " * Acc@1 73.030 Acc@5 97.950\n",
            "lr: [0.042184058920313655]\n",
            "Epoch: [18][  0/391]\tTime  0.236 ( 0.236)\tData  0.198 ( 0.198)\tLoss 6.2241e-01 (6.2241e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [18][ 50/391]\tTime  0.031 ( 0.075)\tData  0.001 ( 0.036)\tLoss 8.4187e-01 (7.2041e-01)\tAcc@1  68.75 ( 75.06)\tAcc@5  98.44 ( 98.54)\n",
            "Epoch: [18][100/391]\tTime  0.058 ( 0.064)\tData  0.034 ( 0.028)\tLoss 6.0738e-01 (7.3092e-01)\tAcc@1  78.12 ( 74.73)\tAcc@5  99.22 ( 98.38)\n",
            "Epoch: [18][150/391]\tTime  0.061 ( 0.059)\tData  0.032 ( 0.024)\tLoss 6.7961e-01 (7.3575e-01)\tAcc@1  76.56 ( 74.44)\tAcc@5  99.22 ( 98.37)\n",
            "Epoch: [18][200/391]\tTime  0.036 ( 0.057)\tData  0.007 ( 0.023)\tLoss 6.8440e-01 (7.3966e-01)\tAcc@1  78.91 ( 74.31)\tAcc@5  97.66 ( 98.40)\n",
            "Epoch: [18][250/391]\tTime  0.039 ( 0.058)\tData  0.006 ( 0.023)\tLoss 7.8729e-01 (7.3867e-01)\tAcc@1  71.88 ( 74.36)\tAcc@5 100.00 ( 98.39)\n",
            "Epoch: [18][300/391]\tTime  0.043 ( 0.059)\tData  0.001 ( 0.025)\tLoss 8.6236e-01 (7.4644e-01)\tAcc@1  67.97 ( 74.13)\tAcc@5  98.44 ( 98.31)\n",
            "Epoch: [18][350/391]\tTime  0.036 ( 0.058)\tData  0.001 ( 0.024)\tLoss 7.6326e-01 (7.4593e-01)\tAcc@1  70.31 ( 74.19)\tAcc@5  96.88 ( 98.26)\n",
            "Test: [ 0/79]\tTime  0.155 ( 0.155)\tLoss 7.5325e-01 (7.5325e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  96.88 ( 96.88)\n",
            "Test: [50/79]\tTime  0.046 ( 0.032)\tLoss 9.2287e-01 (8.0975e-01)\tAcc@1  66.41 ( 72.32)\tAcc@5  97.66 ( 97.98)\n",
            " * Acc@1 72.180 Acc@5 97.970\n",
            "lr: [0.050004999999999994]\n",
            "Epoch: [19][  0/391]\tTime  0.243 ( 0.243)\tData  0.199 ( 0.199)\tLoss 7.4405e-01 (7.4405e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [19][ 50/391]\tTime  0.054 ( 0.073)\tData  0.024 ( 0.036)\tLoss 9.4764e-01 (7.6069e-01)\tAcc@1  66.41 ( 72.95)\tAcc@5  96.88 ( 98.41)\n",
            "Epoch: [19][100/391]\tTime  0.061 ( 0.062)\tData  0.035 ( 0.027)\tLoss 6.9131e-01 (7.7557e-01)\tAcc@1  75.78 ( 72.87)\tAcc@5  98.44 ( 98.23)\n",
            "Epoch: [19][150/391]\tTime  0.064 ( 0.059)\tData  0.036 ( 0.024)\tLoss 1.0113e+00 (7.6436e-01)\tAcc@1  71.09 ( 73.21)\tAcc@5  95.31 ( 98.24)\n",
            "Epoch: [19][200/391]\tTime  0.070 ( 0.057)\tData  0.039 ( 0.023)\tLoss 8.2901e-01 (7.6371e-01)\tAcc@1  71.09 ( 73.17)\tAcc@5  98.44 ( 98.28)\n",
            "Epoch: [19][250/391]\tTime  0.051 ( 0.057)\tData  0.013 ( 0.023)\tLoss 7.5915e-01 (7.6468e-01)\tAcc@1  75.00 ( 73.30)\tAcc@5  99.22 ( 98.27)\n",
            "Epoch: [19][300/391]\tTime  0.036 ( 0.059)\tData  0.007 ( 0.025)\tLoss 7.7977e-01 (7.6398e-01)\tAcc@1  72.66 ( 73.39)\tAcc@5  98.44 ( 98.22)\n",
            "Epoch: [19][350/391]\tTime  0.057 ( 0.058)\tData  0.034 ( 0.024)\tLoss 6.6159e-01 (7.6426e-01)\tAcc@1  78.12 ( 73.49)\tAcc@5  97.66 ( 98.18)\n",
            "Test: [ 0/79]\tTime  0.182 ( 0.182)\tLoss 7.3962e-01 (7.3962e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5 100.00 (100.00)\n",
            "Test: [50/79]\tTime  0.022 ( 0.033)\tLoss 1.0679e+00 (8.8547e-01)\tAcc@1  68.75 ( 70.39)\tAcc@5  95.31 ( 97.89)\n",
            " * Acc@1 70.310 Acc@5 97.830\n",
            "lr: [0.05782594107968634]\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "epochs = []\n",
        "results = []\n",
        "for epoch in range(START_EPOCH, EPOCHS):\n",
        "#    adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "    # train for one epoch\n",
        "    train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "    # evaluate on validation set\n",
        "    val = validate(val_loader, model, criterion)\n",
        "    acc1 = val[0]\n",
        "\n",
        "    # remember best acc@1 and save checkpoint\n",
        "    is_best = acc1 > best_acc1\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': ARCH,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_acc1': best_acc1,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }, is_best)\n",
        "\n",
        "    # save Acc@1 as array for each epoc\n",
        "    epochs.append(epoch)\n",
        "    results.append(val[1].item())\n",
        "    \n",
        "    scheduler.step()\n",
        "    print('lr: ' + str(scheduler.get_last_lr()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "45c4cb64",
      "metadata": {
        "id": "45c4cb64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1737dbe6-4cd6-48ee-ce45-5957d1feb43a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#display the loss over epoch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epochs, results)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss still fluctuates a lot but looks like it has a general trend downards to about 11 epochs and then jumps and needs to come down again. If we were to run for 200 epochs, we would likely see better convergence. "
      ],
      "metadata": {
        "id": "fodeeqsv8CTP"
      },
      "id": "fodeeqsv8CTP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}